cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py': No such file or directory
cp: cannot create regular file '/usr/local/lib/python3.8/dist-packages/deepspeed/ops/adam/cpu_adam.py': No such file or directory
PYTHONGIL=1 python pretrain_gpt.py --num-layers 16 --hidden-size 2048 --num-attention-heads 16 --seq-length 1024 --micro-batch-size 4 --global-batch-size 4 --max-position-embeddings 1024 --train-iters 50 --log-interval 10 --exit-interval 50 --lr-decay-iters 320000 --save ./checkpoints/gpt2 --load ./checkpoints/gpt2 --data-path /shared_ssd_storage/shilonglei/OOC/Megatron-DeepSpeed/examples/data/meg-gpt2-oscar-en-10k_text_document --vocab-file --merge-file --data-impl mmap --distributed-backend nccl --split 949,50,1 --lr 0.00015 --min-lr 0.00001 --lr-decay-style cosine --lr-warmup-fraction .01 --weight-decay 1e-2 --clip-grad 1.0 --log-interval 10 --save-interval 10000 --eval-interval 1000 --eval-iters 1000 --checkpoint-activations --activations-checkpoint-method 'uniform' --activations-checkpoint-num-layers 1 --enable-gl --use-cpu-initialization --gl-world-size 1 --gl-window-size 4 --gl-ray-max-concurrency 12
[2023-08-21 18:10:42,045] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
                       [--hidden-size HIDDEN_SIZE]
                       [--ffn-hidden-size FFN_HIDDEN_SIZE]
                       [--num-attention-heads NUM_ATTENTION_HEADS]
                       [--kv-channels KV_CHANNELS]
                       [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
                       [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
                       [--layernorm-epsilon LAYERNORM_EPSILON]
                       [--apply-residual-connection-post-layernorm]
                       [--openai-gelu] [--onnx-safe ONNX_SAFE]
                       [--bert-no-binary-head]
                       [--attention-dropout ATTENTION_DROPOUT]
                       [--hidden-dropout HIDDEN_DROPOUT]
                       [--weight-decay WEIGHT_DECAY] [--clip-grad CLIP_GRAD]
                       [--adam-beta1 ADAM_BETA1] [--adam-beta2 ADAM_BETA2]
                       [--adam-eps ADAM_EPS] [--sgd-momentum SGD_MOMENTUM]
                       [--micro-batch-size MICRO_BATCH_SIZE]
                       [--batch-size BATCH_SIZE]
                       [--global-batch-size GLOBAL_BATCH_SIZE]
                       [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
                       [--checkpoint-activations]
                       [--distribute-checkpointed-activations]
                       [--activations-checkpoint-method {uniform,block}]
                       [--activations-checkpoint-num-layers ACTIVATIONS_CHECKPOINT_NUM_LAYERS]
                       [--train-iters TRAIN_ITERS]
                       [--train-samples TRAIN_SAMPLES]
                       [--log-interval LOG_INTERVAL]
                       [--exit-interval EXIT_INTERVAL]
                       [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
                       [--tensorboard-dir TENSORBOARD_DIR]
                       [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
                       [--no-bias-dropout-fusion] [--optimizer {adam,sgd}]
                       [--dataloader-type {single,cyclic}]
                       [--no-async-tensor-model-parallel-allreduce]
                       [--seed SEED] [--init-method-std INIT_METHOD_STD]
                       [--init-method-xavier-uniform] [--lr LR]
                       [--lr-decay-style {constant,linear,cosine}]
                       [--lr-decay-iters LR_DECAY_ITERS]
                       [--lr-decay-samples LR_DECAY_SAMPLES]
                       [--lr-warmup-fraction LR_WARMUP_FRACTION]
                       [--lr-warmup-iters LR_WARMUP_ITERS]
                       [--lr-warmup-samples LR_WARMUP_SAMPLES]
                       [--warmup WARMUP] [--min-lr MIN_LR]
                       [--override-lr-scheduler]
                       [--use-checkpoint-lr-scheduler] [--save SAVE]
                       [--save-interval SAVE_INTERVAL] [--no-save-optim]
                       [--no-save-rng] [--load LOAD] [--no-load-optim]
                       [--no-load-rng] [--finetune] [--fp16] [--bf16]
                       [--loss-scale LOSS_SCALE]
                       [--initial-loss-scale INITIAL_LOSS_SCALE]
                       [--min-loss-scale MIN_LOSS_SCALE]
                       [--loss-scale-window LOSS_SCALE_WINDOW]
                       [--hysteresis HYSTERESIS] [--fp32-residual-connection]
                       [--no-query-key-layer-scaling]
                       [--attention-softmax-in-fp32]
                       [--accumulate-allreduce-grads-in-fp32]
                       [--fp16-lm-cross-entropy]
                       [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
                       [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
                       [--model-parallel-size MODEL_PARALLEL_SIZE]
                       [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
                       [--distributed-backend {nccl,gloo}]
                       [--DDP-impl {local,torch}]
                       [--no-contiguous-buffers-in-local-ddp]
                       [--no-scatter-gather-tensors-in-pipeline]
                       [--local_rank LOCAL_RANK]
                       [--lazy-mpu-init LAZY_MPU_INIT]
                       [--use-cpu-initialization]
                       [--empty-unused-memory-level {0,1,2}]
                       [--eval-iters EVAL_ITERS]
                       [--eval-interval EVAL_INTERVAL]
                       [--data-path [DATA_PATH ...]] [--split SPLIT]
                       [--vocab-file VOCAB_FILE] [--merge-file MERGE_FILE]
                       [--vocab-extra-ids VOCAB_EXTRA_IDS]
                       [--seq-length SEQ_LENGTH]
                       [--encoder-seq-length ENCODER_SEQ_LENGTH]
                       [--decoder-seq-length DECODER_SEQ_LENGTH]
                       [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
                       [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
                       [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
                       [--num-workers NUM_WORKERS]
                       [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer}]
                       [--data-impl {lazy,cached,mmap,infer}]
                       [--reset-position-ids] [--reset-attention-mask]
                       [--eod-mask-loss] [--adlr-autoresume]
                       [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
                       [--ict-head-size ICT_HEAD_SIZE]
                       [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
                       [--biencoder-shared-query-context-model]
                       [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
                       [--titles-data-path TITLES_DATA_PATH]
                       [--query-in-block-prob QUERY_IN_BLOCK_PROB]
                       [--use-one-sent-docs]
                       [--evidence-data-path EVIDENCE_DATA_PATH]
                       [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
                       [--retriever-score-scaling]
                       [--block-data-path BLOCK_DATA_PATH]
                       [--embedding-path EMBEDDING_PATH]
                       [--indexer-batch-size INDEXER_BATCH_SIZE]
                       [--indexer-log-interval INDEXER_LOG_INTERVAL]
                       [--num-classes NUM_CLASSES] [--img-dim IMG_DIM]
                       [--num-channels NUM_CHANNELS] [--patch-dim PATCH_DIM]
                       [--log-params-norm] [--log-num-zeros-in-grad]
                       [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
                       [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
                       [--log-timers-to-tensorboard]
                       [--log-batch-size-to-tensorboard]
                       [--no-log-learnig-rate-to-tensorboard]
                       [--no-log-loss-scale-to-tensorboard]
                       [--log-validation-ppl-to-tensorboard]
                       [--log-memory-to-tensorboard] [--enable-l2l]
                       [--enable-gl] [--gl-debug-print]
                       [--gl-window-size GL_WINDOW_SIZE] [--gl-enable-ddp]
                       [--gl-world-size GL_WORLD_SIZE]
                       [--gl-ray-max-concurrency GL_RAY_MAX_CONCURRENCY]
pretrain_gpt.py: error: argument --vocab-file: expected one argument
